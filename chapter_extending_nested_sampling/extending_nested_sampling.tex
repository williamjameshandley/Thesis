\chapter{Extending Nested Sampling}
\label{chap:ens}

\section{Evidence estimates and errors}                            
\label{sec:ens:evidences}

\cite{skilling2006} initially advocated using Monte-Carlo methods to estimate the evidence error, although this requires the storage of the entire chain of dead points, rather than just the subset usually stored for posterior inferences. For high-dimensional problems, the number of dead points is prohibitively large, and cannot be stored.

\cite{MultiNest2} use an alternative method based on the relative entropy (also suggested by~\cite{skilling2006}). 

\cite{Keeton} suggests a more intuitive methodology of estimating the error, and it is this which we use, although it must be heavily adapted for the case of variable numbers of live points and clustering.

\subsection{Basic theory}
\label{sec:ens:basic_theory}

We wish to compute the sum:
%
\begin{equation}
  \ev = \sum\limits_{i} (X_{i-1}-X_{i})\lik_i.
  \label{eqn:ens:app_ev}
\end{equation}
%
However, we do not know the volumes $X_i$ exactly, so we can only make inferences about $\ev$, in terms of a probability distribution $\Prob{\ev}$. In practice, all we need to compute is the mean and variance of this distribution:
\begin{align}
  \text{mean}(\ev) &\equiv \overline{\ev},\\
  \text{var}(\ev) &\equiv \overline{\ev^2}-\overline{\ev}^2.
\end{align}
%
At iteration $i$, the $\nlive$ live points are each uniformly sampled within a contour of volume $X_{i-1}$. The volume $X_i$ will be the largest volume out of $\nlive$ uniform volume samples in volume $X_i$.
Thus $X_i$ satisfies the recursion relation:
%
\begin{align}
  X_i &= t X_{i-1}, \qquad X_0=1, \label{eqn:ens:rr_x} \\
  P(t) &= \nlive t^{\nlive - 1}, \label{eqn:ens:Pt}
\end{align}
%
where the $t$ and $X_{i-1}$ are independent.

It is worth noting that the procedure described below will generate the mean and variance of the distribution, but in fact this is not quite what we want. The evidence is in practice approximately log-normally distributed. Thus, it is better to report the mean and variance of $\log\ev$, defined by:
\begin{align}
  \text{mean}(\log\ev) &= 2\log\overline{\ev} - \frac{1}{2}\log\overline{\ev^2},\\
  \text{var}(\log\ev) &= \log\overline{\ev^2}-2\log\overline{\ev}.
\end{align}


\subsection{Computing the mean evidence}
\label{sec:ens:basic_mean}

While it is possible to take equations~\eqref{eqn:ens:app_ev},\ref{eqn:ens:rr_x} \&~\ref{eqn:ens:Pt} and compute the mean as a general formula~\citep{Keeton}, in the case of clustering this is uninformative. 
In fact, for large-dimensional spaces using the full formula would require storage of a prohibitively large amount of data. The calculation is better accomplished by a set of recursion relations, which update the mean evidence and its error at each step. 

For now, assume that we have $n$ live points currently enclosed by some likelihood contour $\lik$ of volume $X$, and $\ev$ is the last value of the evidence calculated from all of the points that have died so far. By considering~\eqref{eqn:ens:app_ev},\ref{eqn:ens:rr_x}\&\ref{eqn:ens:Pt}, when we kill off the outermost point, we may adjust the values of $\ev$ and $X$ using:
%
\begin{align}                                                         
\ev &\to \ev + (1-t)X\lik,
\label{eqn:ens:rr_Z}
\\
X &\to tX.
\label{eqn:ens:rr_X}
\end{align}
%
Taking the mean of these relations, we may use the facts that $t$ and $X$ are independent random variables and that $P(t) = n t^{n-1}$, to find the recursion relations:
\begin{align}
  \overline{\ev} &\to \overline{\ev} + \frac{1}{n+1}\overline{X}\lik,
  \label{eqn:ens:rr_Zb}
  \\
  \overline{X} &\to \frac{n}{n+1}\overline{X}.
  \label{eqn:ens:rr_Xb}
\end{align}
%

\subsection{Computing the evidence error}
\label{sec:ens:basic_error}
To estimate $\overline{\ev^2}$, we square~\eqref{eqn:ens:rr_Z} and~\eqref{eqn:ens:rr_X} and multiply both together to obtain:
%
\begin{align}
  \ev^2 &\to \ev^2 + 2(1-t)\ev X\lik +  {(1-t)}^2X^2\lik ^2,
  \label{eqn:ens:rr_Z2b}
  \\
  \ev X &\to t\ev X + t(1-t)X^2\lik,
  \label{eqn:ens:rr_ZXb}
  \\
  X^2 &\to t^2X^2.
  \label{eqn:ens:rr_X2b}
\end{align}
%
Note that we now need to keep track of the variable $\ev X$, as these two are not independent.
Taking the averages of the above yields:
\begin{align}
  \overline{\ev^2} &\to \overline{\ev^2} + \frac{2\overline{\ev X}\lik}{n+1} +  \frac{2\overline{X^2}\lik^2}{(n+1)(n+2)},
  \\
  \overline{\ev X} &\to \frac{n\overline{\ev X}}{n+1} + \frac{n\overline{X^2}\lik}{(n+1)(n+2)},
  \\
  \overline{X^2} &\to \frac{n}{n+2}\overline{X^2}.
\end{align}

\subsection{The full calculation}
\label{sec:ens:basic_full}

There are therefore five quantities to keep track of: 
\[ \overline{\ev},\quad\overline{\ev^2},\quad\overline{\ev X},\quad \overline{X},\quad\overline{X^2}.\]
These should be initialised at $\{0,0,0,1,1\}$ respectively, and updated using equations~\eqref{eqn:ens:rr_Zb},\ref{eqn:ens:rr_Z2b},\ref{eqn:ens:rr_ZXb},\ref{eqn:ens:rr_Xb},\ref{eqn:ens:rr_X2b} in that order. In fact, we keep track of the logarithm of these quantities, in order to avoid machine precision errors.





\section{Evidence estimates and errors in clusters}
\label{sec:ens:evidences_clusters}
This analysis follows that of Section~\ref{sec:ens:evidences}. We recommend that you have understood the methods described there before continuing.


Throughout the algorithm, there will in general be $m$ identified clusters. In doing so, we wish to keep track of the volume of each cluster $\{X_1,\ldots,X_m\}$, the global evidence and its error $\ev,\ev^2$ and the local evidences and their errors $\{\ev_1,\ev_1^2\ldots,\ev_m,\ev_m^2\}$. At each iteration, the point with the lowest likelihood $\lik$ will be killed from cluster $p$, ${(1\le p\le m)}$. 


\subsection{Evidence}
\label{sec:ens:cluster_ev}

We thus need to update the global evidence, the local evidence of cluster $p$, and the volume of cluster $p$:
%
\begin{align}
  \ev &\to \ev + (1-t)X_p\lik,
  \label{eqn:ens:rr_ev}\\
  \ev_p &\to \ev_p + (1-t)X_p\lik,
  \label{eqn:ens:rr_evi}\\
  X_p &\to t X_p.
  \label{eqn:ens:rr_Xi}
\end{align}
%
Since $t$ will be distributed with $P(t) = n_p t^{n_p-1}$,
taking the mean of these yields:
%
\begin{align}
  \overline\ev &\to \overline\ev + \frac{\overline X_p \lik}{n_p+1},\\
  \overline\ev_p &\to \overline\ev_p + \frac{\overline X_p \lik}{n_p+1},\\
  \overline X_p &\to \frac{n_p\overline X_p }{n_p+1}. 
\end{align}
%
Keeping track of $\{\overline{\ev},\overline{\ev_p},\overline{X_p},p=1\ldots m\}$ and updating them using the recursion relations in the order above will produce a consistent evidence estimate for both the local and global evidence errors.


\subsection{Evidence errors}
\label{sec:ens:cluster_err}

We must also keep track of the local and global evidence errors. Taking the square of equations~\eqref{eqn:ens:rr_ev} \&~\ref{eqn:ens:rr_evi} yields:
%
\begin{align}
  \ev^2 &\to \ev^2 + 2 (1-t) \ev X_p \lik +  {(1-t)}^2 X_p^2 \lik^2, \\
  \ev_p^2 &\to \ev_p^2 + 2 (1-t) \ev_p X_p \lik +  {(1-t)}^2 X_p^2 \lik^2.
\end{align}
%
We can see that we're going to need to keep track of $\{\overline{\ev X_p}, \overline{\ev_p X_p}, \overline{X_p^2}\}$ in addition to $\{ \overline{\ev^2}, \overline{\ev_p^2} \}$. Taking various multiplications of equations~\eqref{eqn:ens:rr_ev},~\ref{eqn:ens:rr_evi} \&~\ref{eqn:ens:rr_Xi} finds:
%
\begin{align}
  \ev X_p   &\to t \ev X_p + (1-t)t X_p^2 \lik, \\
  \ev X_q   &\to   \ev X_q + (1-t) X_p X_q \lik \qquad (p\ne q),   \\
  \ev_p X_p &\to t \ev_p X_p + (1-t)t X_p^2 \lik, \\
  X_p^2     &\to t^2 X_p^2, \\
  X_p X_q   &\to t   X_p X_q.
\end{align}
%
Taking the mean of the above yields the recursion relations:
\begin{align}
  \overline{\ev^2} &\to \overline{\ev^2} + \frac{2\overline{\ev X_p}\lik_p}{n_p+1}  + \frac{2\overline{X_p^2}\lik^2}{(n_p+1)(n_p+2)}, \\
  \overline{\ev_p^2} &\to \overline{\ev_p^2} + \frac{2\overline{\ev_p X_p}\lik}{n_p+1}  + \frac{2\overline{X_p^2}\lik^2}{(n_p+1)(n_p+2)}, \\
  \overline{\ev X_p} &\to \frac{n_p\overline{\ev X_p}}{n_p+1}  + \frac{n_p\overline{X_p^2} \lik}{(n_p+1)(n_p+2)},   \\
  \overline{\ev X_q} &\to \overline{\ev X_p}  + \frac{\overline{X_p X_q} \lik}{(n_p+1)} \qquad (q\ne p),  \\
  \overline{\ev_p X_p} &\to \frac{n_p\overline{\ev_p X_p}}{n_p+1}  + \frac{n_p\overline{X_p^2} \lik}{(n_p+1)(n_p+2)},   \\
  \overline{X_p^2} &\to \frac{n_p\overline{X_p^2}}{n_p+2}, \\
  \overline{X_p X_q} &\to \frac{n_p\overline{X_p X_q}}{n_p+1}.
\end{align}
Keeping track of 
\[\{\overline{\ev^2},\overline{\ev_p^2},\overline{\ev X_p},\overline{\ev_p X_p},\overline{X_p^2},\overline{X_p X_q},p,q=1\ldots m\},\]
and updating them using the recursion relations in the order above will produce a consistent estimate for the local and global evidence errors.



\subsection{Cluster initialisation}
\label{sec:ens:cluster_init}
All that remains is to initialise the clusters correctly at the point of creation.

The starting initialisation of the evidence and volume is reasonable, there will be only a single cluster with volume $1$, and all evidence related terms $0$. At some point (possibly at the beginning, depending on the prior), the live points will split into distinct clusters, and the local volumes and evidences will need to be re-initialised.

At the point of splitting a cluster into sub-clusters, we partition the $n$ live points into a $N$ new clusters, with $\{n_1,\ldots,n_N\}$ live points in each. If the volume of the splitting cluster is $X_p$ initially, we need to know how to partition this volume into $\{X_1,\ldots,X_N\}$. If the points are drawn uniformly from the volume, then the $n_i$ will depend on the volumes via a multinomial probability distribution:
%
\begin{equation}
  P(\{n_i\}|X_p,\{X_i\}) \propto {X_1}^{n_1} \ldots X_N^{n_N}.
\end{equation}
%
We however want to know the probability distributions of the $\{X_i\}$, given the $\{n_i\}$. We can invert the above with Bayes' theorem, using an (improper) logarithmic prior on the volumes subject to the constraint that they sum to $X_p$:
%
\begin{equation}
  P(\{X_i\}|X_p) \propto \frac{\delta(X_1+\cdots+X_N-X_p)}{X_1\cdots X_N}.
\end{equation}
%
Doing this shows the posterior $P(\{X_i\}|X_p,\{n_i\})$ is a Dirichlet distribution with parameters $\{n_i\}$. More importantly, we can use this to compute the means and correlations for the volumes $\{X_i\}$:
%
\begin{align}
  \overline{X_i}&=  \frac{n_i}{n} \overline X_p, \\
  \overline{X_i^2}&= \frac{n_i(n_i+1)}{n(n+1)} \overline{X_p^2}, \\
  \overline{X_i X_j} &= \frac{n_i n_j}{n(n+1)} \overline{X_p^2}, \\
  \overline{X_i Y} &= \frac{n_i}{n} \overline{X_p Y} \qquad Y\in \{Z,Z_p,X_q\}.
\end{align}
%
The first equation recovers the intuitive result that the volume should split as the fraction of live points. Note, however that this requires a logarithmic prior. The third shows us that since $\overline{X_i X_j}\ne\overline{X_i}\:\overline{X_j}$, the volumes are correlated at the splitting. This is to be expected.

We also need to initialise the local evidences and their errors. A consistent approach is to assume that the evidences also split in proportion to the cluster distribution of live points. Following the same reasoning as above, we find that:
\begin{align}
  \overline{Z_i}&=  \frac{n_i}{n} \overline Z_p \\
  \overline{Z_i X_i}&= \frac{n_i(n_i+1)}{n(n+1)} \overline{Z_p X_p} \\
  \overline{Z_i^2}  &= \frac{n_i(n_i+1)}{n(n+1)} \overline{Z_p^2} \\
\end{align}

Thus, at cluster splitting, all of the new local evidences, volumes and cross correlations are initialised according to the above.

This completes the mechanism for keeping track of the local and global evidences, their errors, and the local cluster volumes.


\section{Further generalisations}

It's worth noting that the procedures in the previous sections are amenable to a great many generalisations. We give a few examples in order to demonstrate this

First, if one desires a higher order of accuracy, one may update the rather simple quadrature in equation~\eqref{eqn:ens:app_ev} to:
\begin{equation}
  \ev = \sum\limits_{i} (X_{i-1}-X_{i}) \times \frac{1}{2}(\lik_i+\lik_{i-1}).
  \label{eqn:ens:app_trap_ev}
\end{equation}
In this case, the approach is identical to the method described above, one merely replaces the likelihood value with that of the average value of the last two likelihoods $(\lik_i + \lik_{i-1})/2$.
It is better to phrase the trapezoidal rule as in equation~\eqref{eqn:ens:app_trap_ev} as oppose to the more traditionally cited:
\begin{equation}
  \ev = \sum\limits_{i} \frac{1}{2}(X_{i-1}-X_{i+1}) \lik_i.
  \label{eqn:ens:app_trap_normal_ev}
\end{equation}
Equations~\eqref{eqn:ens:app_trap_ev} and~\eqref{eqn:ens:app_trap_normal_ev} are equivalent, but the first formulation can be calculated at run time, whereas the second requires knowledge of future values of $X_{i}$, which tends to result in programs with a lot of awkward bookkeeping.

Second, if one wishes to re-phrase the evidence integral:
\begin{equation}
  \ev = \int \lik \: dX = \int X\lik \: \d{\log X}
\end{equation}
then the trapezoidal quadrature must be modified:
\begin{equation}
  \ev = \sum\limits_{i} (\log X_{i-1}-\log X_{i}) \times \frac{1}{2}(\lik_i X_i+\lik_{i-1} X_{i-1}).
\end{equation}
This therefore amount to an update of:
\begin{align}
  \ev &\to \ev + X \times \frac{1}{2}\left(t\log\frac{1}{t}\lik_i + \log\frac{1}{t}\lik_{i-1} \right)
  \\
  X &\to t X.
\end{align}
The analysis then proceeds exactly as before. 

In general, when playing with various formulations, it suffices to know the averages:
\begin{equation}
\left\langle t^a{(1-t)}^b \right\rangle = n\frac{b!(a+n-1)!}{(a+b+n)!},
\end{equation}
and:
\begin{equation}
  \left\langle t^a{\left( \log1/t \right)}^b \right\rangle 
  = 
  \frac{n\: b!}{{(n+a)}^{b+1}}.
\end{equation}
