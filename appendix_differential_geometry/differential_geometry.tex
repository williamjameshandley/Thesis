\chapter{Differential Geometry}
\label{chap:dg}

The application and understanding of Einstein's theory of general relativity requires an understanding of curved spacetime.

\begin{figure}[ht]
  \ifdefined\lightweight{}
  \else
  \centerline{%
    \input{plots/differential_geometry}
  }
  \fi
  \caption{\label{fig:dg:differential_geometry}}
\end{figure}

\section{Coordinates}
Curved spacetime is a collection of events (or points) $p$ in a manifold $\mathcal{M}$. Semi-global patches of it may have their events labelled with coordinates $x^a = x^a(p)$, which are always ``upstairs'' indexed. Since we are in $1+3$ spacetime, typically, $a=0$ will denote a time-like coordinate, and $a=1,2,3$ space-like coordinates.

Technically, a coordinate is a {\em scalar field}. This is a real number $\phi=\phi(p)$ defined for each point $p\in \mathcal{M}$. A coordinate {\em system\/} is therefore a set of scalar fields which provide a one-to-one semi-global mapping from $\mathcal{M}$ to $\mathbb{R}^4$.

We will suppress indices on coordinates when the meaning is clear, for example in the case of a scalar field $\phi$ we write:
\begin{equation}
  \phi(x^a) = \phi(x)
  \label{eqn:dg:scalar_supress}
\end{equation}


%In general, the points $p\in \mathcal{M}$ will not form a vector space, and they cannot be added to one another in a meaningful way. Whilst there may be a physically meaningful addition and scaling of points in flat spacetimes, in the more general case there is not. For example, consider the manifold defined by the surface of the two-dimensional sphere $\mathcal{M}=S^2$. It is not possible to find an addition which is commutative and scalable

\section{Vectors}



This coordinate system defines a {\em contravariant\/} set of basis vectors at each point $\ve_a = \ve_a(p)$. Here the basis vector $\ve_a(p)$ is tangent to the coordinate curve:
\begin{equation}
  \gamma^a = \{x^b=\mathrm{const} : a\ne \nu\},
  \label{eqn:dg:coordinate_curve}
\end{equation}
at the point $p$.\footnote{Experienced readers will know that this notion is normally regarded as far from trivial.}
  Vectors at {\em at a given point\/} $p$ form a vector space which may be summed and scaled. Thus, a general vector $\vv$ may be written as a linear combination of basis vectors $\vv = v^a \ve_a$, where we use Einstein's summation convention that repeated upstairs and downstairs indices should be summed over. In addition, these vectors are equipped with a dot-product $\vv\cdot\vw$ which encapsulates the geometry of the vectors; namely spacetime angles and magnitudes. If a coordinate system is chosen, then:
\begin{equation}
  \vv\cdot\vw = (v^a\ve_a)\cdot(w^b\ve_b) = v^a (\ve_a\cdot\ve_b)w^b =  v^a g_{ab} w^b
  \label{eqn:dg:dot_product}
\end{equation}
where 
\begin{equation}
  g_{ab} = \ve_a\cdot\ve_b
  \label{eqn:dg:metric_def}
\end{equation}
is the {\em metric}, and is the dot product at a given point, expressed in a specific coordinate system.

Once a dot product is defined, it is convenient to define a set of {\em covariant\/} basis vectors $\{\ve^a\}$ via:
\begin{equation}
  \ve^a \cdot \ve_b = \delta^a_b 
  \qquad\Leftrightarrow\qquad
  \ve^a = g^{ab} \ve_b,
  \label{eqn:dg:covariant_basis}
\end{equation}
where $g^{ab}$ is the upstairs version of \eqref{eqn:dg:metric_def}, and by linear algebra is the matrix inverse of $g_{ab}$. Now that a dot product is defined, the covariant basis vector $\ve^a$ can be thought of as orthogonal to the coordinate surface $x^a=\mathrm{const}$. Materials scientists should recognise the covariant basis as the reciprocal basis. 

We can choose to write vectors in either the covariant or contravariant basis:
\begin{equation}
  \vv=v^b \ve_b = v_a \ve^a 
  \qquad\Rightarrow\qquad
  v_a = g_{ab} v^b.
  \label{eqn:dg:covariant_vector}
\end{equation}
The metric can therefore be used to ``raise'' or ``lower'' indices of vectors. The fact that the metric performs this operation should be intuitively obvious, since raised or lowered indexed vectors correspond to placing the vector in the covariant or contravariant coordinate system, whose definition \eqref{eqn:dg:covariant_basis} is defined via the dot-product.

\section{Tensors}
A Tensor $T$ is a multi-linear mapping of vectors onto a scalar at a given point:
\begin{align}
  \uT 
  &= 
  \uT(\va,\:\vb,\ldots,\:\vz)
  &
  \nonumber
  \\
  &= 
  \uT(a^\alpha\ve_\alpha,\:b^\beta\ve_\beta,\ldots,z^\zeta\ve_\zeta)  
  &\text{vectors in coordinate basis}
  \nonumber
  \\
  &= 
  a^\alpha b^\beta \ldots z^\zeta \: 
  \uT(\ve_\alpha,\:\ve_\beta,\:\ldots,\:\ve_\zeta)  
  &\text{apply multi-linearity}
  \nonumber
  \\
  T_{\alpha,\beta,\ldots,\zeta} 
  &=  
  T(\ve_\alpha,\:\ve_\beta,\:\ldots,\:\ve_\zeta)
  &\text{define a new set of numbers}
  \nonumber
  \\
  \Rightarrow\qquad
  \uT
  &= 
  a^\alpha b^\beta \ldots z^\zeta \: 
  T_{\alpha,\beta,\ldots,\zeta}
  &
  \nonumber
\end{align}
Upstairs components of a vector are defined similarly. It is interesting (although not surprising) that the metric is also a tensor, albeit a very special one which defines the geometry. The mapping is coordinate independent, but components are produced when a specific coordinate basis is chosen for each ``slot'' (each of which may be covariant or contravariant).

A mapping of a rank $m$ tensor onto a rank $n$ tensor may be produced from a rank $m+n$ tensor by leaving $n$ of the slots blank. In this sense, a vector can be considered as a rank $1$ tensor, its linear action defined by the dot product.

\section{Calculus}
So far, all we have discussed is a way of dealing with vectors whose bases are defined by an arbitrary coordinate system. The maths becomes interesting when we start to ask questions about how these vectors change from point to point on the manifold.

For scalar fields, the partial derivative is easy to define and calculate. Taylor's theorem:
\begin{equation}
  \phi(x+dx) = \phi(x) + d\vx\cdot\nabla\phi(x),
  \label{eqn:dg:taylor_scalar}
\end{equation}
serves as a powerful definition for the derivative $\nabla$.

For vectors it is not so simple:
\begin{equation}
  \vv(x+dx) = P_{x}^{x+dx} \left[ \vv(x) + (d\vx\cdot\nabla)\vv(x)  \right]
  \label{eqn:dg:taylor_vector}
\end{equation}
in components:
\begin{align}
  \ve_a(x+dx) &= P_{x}^{x+dx} \left[ \ve_a(x) + (d\vx\cdot\nabla)\ve_a(x)  \right] 
  \label{eqn:dg:taylor_vector_comp} \\
  &= P_{x}^{x+dx} \left[ \ve_a(x) + dx^{b}\TG{c}{ab}\ve_c(x)  \right] 
  \label{eqn:dg:taylor_vector_comp} \\
\end{align}



\begin{equation}
  v^a(x+dx)\ve_a(x+dx) = P_{x}^{x+dx} \left[ v^a(x)\ve_a(x) + dx^b\nabla_b(v^a(x)\ve_a(x))  \right]
  \label{eqn:dg:taylor_scalar}
\end{equation}

\begin{equation}
  v^a(x+dx) = v^a(x) + dx^b\left( \partial_b v^a(x) + \TG{a}{cb}v^c(x) \right)
  \label{eqn:dg:taylor_vector}
\end{equation}


A vector is a geometrical object with magnitude and length. Intuition is usually introduced in terms of position vectors, where operationally a vector is what is needed to carry a point $A$ to point $B$.\footnote{In Latin, vector means ``one who carries''}. This is only a valid interpretation in flat space, as in curved space (such as the surface of the sphere) it is impossible to define a ``step in space'' such that those steps form a set of vectors (a vector space) which may be added and multiplied in the usual manner. 

A given coordinate system

In curved space, vectors may only be added {\em locally}. Each point $p\in \mathcal{M}$ defines its own vector space $T_p$. If we travel from a point $p$ to a point $q$ along a curve $\gamma: \mathbb{R}\to \mathcal{M}$, we must find a way $\Gamma$ of associating the two spaces, such that:
\begin{align}
  \Gamma(\gamma)^t_s &: T_{\gamma(s)} \to T_{\gamma(t)} \\
  \Gamma(\gamma)^s_s &= I \\
  \Gamma(\gamma)^t_u &\circ \Gamma(\gamma)^u_s =  \Gamma(\gamma)^t_s.
\end{align}
Note that this will depend on the choice of curve $\gamma$.  In addition, one should also demand that this mapping be in some sense smooth.

In particular, if we consider this association in the limit of an infinitesimal curve, we may write the small change in position as a vector $d\vx = dx^a \ve_a$. We should therefore find that each vector satisfies:

For a vector field:
\begin{equation}
 \vv(x) = v^a(x)\ve_a(x),
  \label{eqn:dg:vector_field}
\end{equation}
the problem is not so straightforward. First we now have two dependencies on position $x$, one in the field's components, and one in its basis vectors. Worse, we have no means of subtracting or adding vectors at different points in a manifold, so the traditional definition using ``small changes'' is useless. Nonetheless, if we attempt to find the partial derivative of this field:
\begin{equation}
  \partial_a\vv = \partial_a(v^b\ve_b) \equiv (\partial_a v^b) \ve_b + v^b \partial_a\ve_b
  \label{eqn:dg:partial_expand}
\end{equation}
we find that the only bit that we should be confused about is the final term involving the partial derivative of the basis vector. We have no means of computing this (yet), but it is certainly true that it will be a linear combination of the basis vectors:
\begin{equation}
  \partial_a\ve_b = \TG{c}{ba} \ve_c.
  \label{eqn:dg:gamma_def}
\end{equation}
The $\Gamma$ coefficients here are termed {\em Christoffel symbols}, and despite the notation are not the components of a tensor. We may also compute $\partial_a \ve^b$ by considering:
\begin{align}
  0 = \partial_a \delta^b_c &= \partial_a (\ve^b\cdot\ve_c)  
  \nonumber\\
  &= (\partial_a\ve^b)\cdot\ve_c + \ve^b\cdot(\partial_a\ve_c)
  \nonumber \\
  &= (\partial_a\ve^b)\cdot\ve_c + \ve^b\cdot\TG{d}{ca} \ve_d 
  \nonumber \\
  &= (\partial_a\ve^b)\cdot\ve_c + \TG{d}{ca} \delta^b_d
  \nonumber \\
  &= (\partial_a\ve^b)\cdot\ve_c + \TG{b}{ca} 
  \nonumber \\
  &= (\partial_a\ve^b)\cdot\ve_c + \TG{b}{ca} 
  \nonumber \\
  \Rightarrow \partial_a\ve^b
  &=  - \TG{b}{ca} \ve^c
  \label{eqn:dg:chris_deriv}
\end{align}
With this, we may define a new {\em covariant derivative\/} for vector fields such that:
\begin{align}
  \partial_a\vv &= (\nabla_a v^b) \ve_b = (\nabla_a v_b) \ve^b,
  \\
  \nabla_a v^b &= \partial_a v^b + \TG{b}{ca} v^c,
   \\
   \nabla_a v_b &= \partial_a v_b - \TG{c}{ba} v_c.
\end{align}
Tensors may be defined similarly so that
\begin{align}
  \partial_a \uT &= (\nabla_a T^{bc}),
  \\
  \nabla_a T^{bc} &= \partial_a T^{bc} + \TG{b}{da}T^{dc} + \TG{c}{da}T^{bd},
\end{align}
with similar formulae for downstairs components.

All that remains is to find out what the Christoffel symbols actually are. Demanding metric compatibility and torsionlessness yields:
\begin{equation}
  \TG{a}{bc} = \frac{1}{2}g^{ad}
  \left( 
  \partial_b g_{dc} 
  +\partial_c g_{bd} 
  -\partial_d g_{bc} 
  \right)
  \label{eqn:dg:christoffel}
\end{equation}

We define the curvature tensor as
\begin{align}
  \tensor{R}{^d_{abc}} v_d &= \left( \nabla_{bc} - \nabla_{cb} \right) v_a
  \label{eqn:dg:riemann_def}\\
  \tensor{R}{^d_{abc}} &= \partial_b \TG{d}{ac} - \partial_c \TG{d}{ab} + \TG{e}{ac} \TG{d}{ab} - \TG{e}{ab} \TG{d}{ec}
\end{align}

The Ricci tensor is defined as:
\begin{equation}
  R_{ab} = \tensor{R}{^d_{dab}}
  \label{eqn:dg:ricci_tensor}
\end{equation}

The Ricci scalar is defined as:
\begin{equation}
  R = R^a_a
  \label{eqn:dg:ricci_scalar}
\end{equation}



\section{FRW quantities}
\begin{align}
  \TG{0}{00} &= \TG{0}{0i} = \TG{0}{i0} = 0 \\
  \TG{0}{ij} &= \delta_{ij} \dot{a}a \\
  \TG{i}{0j} &= \TG{j}{i0} = \delta_{ij} \frac{\dot{a}}{a} \\
  R_{00} &= -3 \frac{\ddot{a}}{a} \\
  R_{ij} &= -3 \delta_{ij}\left[ 2{\dot{a}}^2 + a \ddot{a} \right] \\
  R      &=  6 \left[ \frac{\ddot{a}}{a} + {\left( \frac{\dot{a}}{a} \right)}^2 \right] \\
  G_{00} &= 3 {\left( \frac{\dot{a}}{a} \right)}^2
\end{align}



