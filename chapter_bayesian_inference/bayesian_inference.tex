\chapter{Bayesian Inference}
\label{chap:bay}
%\includegraphics[width=\textwidth]{chapter_bayesian_inference/figures/bayesian_evol}

\epigraph{We anticipate the sun will rise tomorrow, not just because it has always done so far, but because this is predicted by {\em models}, which accord with {\em data}. Any perceived failure of the sun to rise would more likely be a hallucination.}{\davidmackay{}}

Despite popular conception, science is not the search for truth. Scientists instead concern themselves with the construction descriptive and predictive models that have their relative merit determined using data.
The natural language to encapsulate these ideas is that of Bayesian probability. This chapter reviews the concepts and notation of this ``meta-science''.

\section{Probability}
\label{sec:bay:prob}

Probability is the mathematical language of uncertainty. 
A numerical weighting is assigned to all events, which roughly corresponds to the ``chance'' that such an event would occur. An event $E\subset \Omega$ is defined as a subset of all possible outcomes $\Omega$. Probability is defined as being additive over the set of all possible events, such that:
\begin{equation}
  A\cap B = \phi \Leftrightarrow \text{$A$, $B$ disjoint} \Leftrightarrow \Prob{A} + \Prob{B} = \Prob{A\cup B}.
\end{equation}
\johnskilling{} has derived these properties of probability from measurement-theoretical grounds~\citep[chap. 1]{Bayesian_methods_in_cosmology}, which provides a more intuitive backing to the standard Kolmogorov axioms.

\subsection{Bayes' theorem}

\begin{figure}[tp]
  \centering
  \tikzsetnextfilename{bayes_theorem}
  \includegraphics[width=\textwidth]{chapter_bayesian_inference/plots/bayes_theorem.tikz}
  \caption{Diagrammatic Bayes' theorem. Nodes indicate the occurrence of an event. Edges indicate the probability of an event occurring, and are multiplicative in the direction of the arrows. Bayes' theorem~\protect\eqref{eqn:bay:bayes} may be derived by equating the path $A\to A\cap B$ with $A\to\Omega\to B\to A\cap B$}\label{fig:bay:bayes_theorem}
\end{figure}


%\begin{figure}[tp]
%  \centerline{%
%    \input{chapter_bayesian_inference/plots/probability}
%  }
%  \caption{Beer-mat mathematics. This diagram is how \edjaynes{} explained Bayes' theorem to a young \stevegull{}. Out of the entire sample space $\Omega$, if we observe $A$ to be true (blue region), then the probability of $B$ given that we know $A$ is simply $\Prob{A\cap B}/\Prob{A}$. Bayes' theorem then follows by rearrangement and symmetry.}
%\end{figure}


The conditional probability of event $B$ given event $A$ is defined via:
\begin{equation}
  \Probc{B}{A} = \frac{\Prob{A\cap B}}{\Prob{A}}.
  \label{eqn:bay:cond}
\end{equation}
Multiplying both sides by $\Prob{A}$, and noting the symmetrical alternative:
\begin{equation}
  \Probc{B}{A}\Prob{A} = \Prob{A\cap B} = \Probc{A}{B}\Prob{B},
  \label{eqn:bay:sym}
\end{equation}
one may then derive Bayes' theorem:
\begin{equation}
  \Probc{B}{A} = \frac{\Probc{A}{B}\Prob{B}}{\Prob{A}}.
  \label{eqn:bay:bayes}
\end{equation}
Bayes' theorem indicates how to reverse the conditionality in a probability distribution $\Probc{A}{B}$: one first deconditions by multiplying by $\Prob{B}$ and then reconditions via the normalising constant $\Prob{A}$. Figure~\ref{fig:bay:bayes_theorem} extends this to all possibilities for two events.
With the algebra of conditionals, probability is the only measurable quantity with both a closed multiplication and addition.

\section{Bayesian vs.\ Frequentist}
\label{sec:bay:bayesian_frequentist}

There are two fundamental types of probability: {\em Aleatoric\/} and {\em Epistemological}. Aleatoric systems are genuinely random, for example: the flip of a coin, or the click on a Geiger counter. Epistemological probability governs systems where the randomness is associated with a subjective lack of knowledge. 

Along with this, there are two interpretations of probability and the meaning of chance; {\em Bayesian\/} and {\em Frequentist}.\footnote{Personally, I disapprove of this terminology. I prefer the terms ``correct'' and ``unhelpfully restrictive''.}
The Frequentist school of thought defines probability as:
\begin{description}
  \item[Frequentist Probability:]``The limiting relative frequency of an event.''
\end{description}
If a coin has $\Prob{\mathrm{Head}}=\frac{1}{2}$, then if you were to toss it an arbitrarily large number of times, the fraction of events that are heads would get closer and closer to $\frac{1}{2}$. This is the version of probability that most people encounter early in their mathematical education. 

This definition applies relatively well to Aleatoric systems (since an experiment can often be run a large number of times). However, for epistemological systems, it is all but useless. The frequentist solution is therefore to disregard the latter kind of probability.

For example, most people's experiences with probability will likely involve betting scenarios. For example, before prince George was born, bookmakers took bets on whether William and Kate's baby was a boy or a girl. Here it is obvious that the betting odds should be approximately $50:50$,\footnote{Betting odds of $a:b$ against indicates a probability of success of $\frac{b}{a+b}$.}\footnote{In fact the probability of a boy being conceived across the population is roughly 51\%, in order to biologically account for male infant mortality. An individual's probability of conceiving a boy or a girl will vary from person to person and in time.}\footnote{A good bookmaker will obviously take this into account, and give you slightly worse odds in order to ensure their profit.} but it should be also be clear that these odds do not refer to an event that can be repeated an arbitrarily large number of times.

The alternative definition of probability is {\em Bayesian\/}:
\begin{description}
  \item[Bayesian Probability:] ``A degree of belief that an event will occur.'' 
\end{description}
Note that Bayesian probability is {\em subjective}, it matters who's degree of belief one is considering. Probabilities are assigned only with a given state of knowledge. Effectively, Bayesians place aleatoric and epistemological probability under the same umbrella. 

Pure mathematics deals in relative truth or falsity, i.e.\ given initial assumptions, all statements are assigned to the set $\{F,T\}\equiv\{0,1\}$. Bayesian probability can be thought of as a blurring of this process, namely from initial assumptions, various conclusions are assigned a number from the continuum between $[0,1]$.

\section{An example: biased coins}
As an concrete introduction to the terminology, we will consider the case of determining whether a coin is biased.
For example, assume that you toss a coin $N=20$ times, and observe a dataset of $\data = 16$ heads. Is this enough to cause us to doubt the fairness of the coin?

The standard model $\model_0$ is that a coin toss consists of a binomial trial with probability $\frac{1}{2}$. Elementary probability tells us that the chance of getting this data is:
\begin{equation}
  \Probc{\data}{\model_0}= {^{N}C_\data} {\left( \frac{1}{2} \right)}^{N}  = 4.6\times 10^{-3}.
  \label{eqn:bay:M0}
\end{equation}
Note that as should be expected, the chance of obtaining this exact dataset is small. 

If we allow for the possibility that the coin could be any probability $p$, we can encapsulate this in a second model $\model_1$.
\begin{equation}
  \Probc{\data}{p,\model_1}= {^{N}C_\data}\: {p}^{\data} {\left(1 - p \right)}^{N-\data}.
  \label{eqn:bay:M1}
\end{equation}
This second model is not ideal, since we haven't specified the parameter $p$. If we choose $p=\frac{1}{2}$ we recover $\model_0$. It is tempting to choose $p$ such that the probability of getting the data is maximised. This would be an example of a {\em maximum likelihood\/} approach. In this case, the method indicates\footnote{Proof left as exercise to the reader.} we should choose $p = \frac{\data}{N}$. 

Maximum likelihood has its drawbacks, particularly in the case of a low volume of data. After $N=1$ toss, it seems a little premature to choose either $p=1$ or $p=0$ depending on whether we see a head or a tail. Instead picking a specific value of $p$, one could instead ``spread your bets'' and consider several different values of $p$. The full generalisation of this is to consider a continuum of models, and define an initial probability distribution on $p$, $\Probc{p}{\model_1}$. This can be interpreted as our initial assumptions on the value of $p$, or our initial degree of belief in its value.
A natural assumption is to try and be as unbiased as possible and assume that $p$ is equally likely to take any value in between $0$ and $1$. We therefore have:
\begin{equation}
  \Probc{p}{\model_1}=\left\{
  \begin{array}{lr}
    1 &:0\le p\le1\\
    0 &:\text{otherwise.}\\
  \end{array}
  \right.\label{eqn:bay:prior1}
\end{equation}
With this distribution, we can work out what the overall probability of obtaining the dataset $\data$ is, by marginalising over all values of $p$:
\begin{align}
  \Probc{\data}{\model_1} 
  &= \int \Probc{\data}{p,\model_1}\Probc{p}{\model_1}\: \d{p}, \\
  &= \int_0^1 {^{N}C_\data}\: {p}^{\data} {\left(1 - p \right)}^{N-\data}\: \d{p}, \\
  &= \frac{1}{n+1} = 4.8\times10^{-2}.
  \label{eqn:bay:marg}
\end{align}
This is quite telling, because our initial choice~\eqref{eqn:bay:prior1} for $\Probc{p}{\model_1}$ indicates that we expect all data sets with equal probability (independent of $\data$). This is therefore a ``minimally suspicious'' choice in spread.

We now have two equations~\eqref{eqn:bay:M0}~\&~\eqref{eqn:bay:marg} which detail the probability of getting the dataset $\data$ given the choice of model. However, what we are really after is the probability of the model, given the dataset $\Probc{\model}{\data}$. To compute this, we use Bayes' theorem:
\begin{equation}
  \Probc{\model}{\data} = \frac{\Probc{\data}{\model}\Prob{\model}}{\Prob{\data}}.
  \label{eqn:bay:bay_md}
\end{equation}
In order to complete the calculation, we must assign a probability to each model. A natural choice is to consider $\Prob{\model_0}=\Prob{\model_1}=\frac{1}{2}$. $\Prob{\data}$ is just a normalising constant, computed as:
\begin{equation}
  \Prob{\data} = \sum_i \Probc{\data}{\model_i}\Prob{\model_i}, 
  \label{eqn:bay:norm}
\end{equation}
so we may finally compute:
\begin{equation}
  \Probc{\model_0}{\data} = 0.088, \qquad
  \Probc{\model_1}{\data} = 0.922.
  \label{eqn:bay:comp}
\end{equation}
In other words, a betting man would give the odds of the coin being unbiased $\model_0$ as approximately $10:1$. Another way of thinking of this is that $\model_1$ is ten times better at describing the data than $\model_0$.

$\model_1$ can go further however. We may also use Bayes' theorem to compute:
\begin{equation}
  \Probc{p}{\data,\model_1} = \frac{\Probc{\data}{p,\model_1}\Probc{p}{\model_1}}{\Probc{\data}{\model_1}}.
  \label{eqn:bay:bay_p}
\end{equation}
This here gives us the distribution on $p$ {\em given the data}. Namely, how the data should update our ``spread bet''. We already have the ingredients for the above construction from equations~\eqref{eqn:bay:M1},~\eqref{eqn:bay:prior1},~\&~\eqref{eqn:bay:marg}, and given the uniform prior, we find our updated bet on $p$ is proportional to $p^\data{(1-p)}^{N-\data}$, a function of $p$. This is a beta distribution, and is indicated in Figure~\ref{fig:bay:beta}.

\begin{figure}[tp]
  \centering
  \tikzsetnextfilename{beta_function}
  \includegraphics[width=\textwidth]{chapter_bayesian_inference/plots/beta_function.tikz}
  \caption{%
  Our beta-function posterior degree of belief in the bias of the coin $p$, after observing a dataset of $(\data,n)=(16,20)$, and assuming a uniform prior on $p$.\label{fig:bay:beta}
}
\end{figure}

\section{Parameter estimation \& model comparison}
\label{sec:bay:model_comp}
We shall now take the concepts of the previous section, and put them in a general setting. 

The typical problem of science is the construction of a model $\model$ in order to explain some dataset $\data$. In general, scientific models have a set of continuous parameters $\params_\model$, where $\params_\model$ is normally multi-dimensional, and may contain a variety of parameter types such as integers, vectors, tensors and more exotic components.

Elementary probability theory then enables us to calculate the probability of the data, given the choice of model along with a specific parameter choice.
\begin{equation}
  \lik\equiv\Probc{\data}{\params_\model,\model}.
  \label{eqn:bay:lik_def}
\end{equation}
This distribution is called the {\em likelihood}, which is denoted with a calligraphic~$\lik$ to differentiate between rapidly proliferating conditional probabilities.\footnote{This should not be confused with a Lagrangian, also denoted with $\mathcal{L}$.} It is also clearer in many situations to supress explicit data, model and/or parameter-dependence of the likelihood, and instead write:
\begin{equation}
  \Probc{\data}{\params_\model,\model}
  \equiv
  \lik_\model(\params_\model)
  \equiv
  \lik(\params)
  \equiv
  \lik_\model
  \equiv
  \lik.\nonumber
\end{equation}
This is a generic overloading technique which I utilise throughout this thesis.

In order to perform Bayesian inference, another requirement of the model $\model$ is that it must specify an initial degree of knowledge of the parameters:
\begin{equation}
  \prior\equiv\Probc{\params_\model}{\model}.
  \label{eqn:bay:prior_def}
\end{equation}
Since no model occurs in isolation, it is generally not difficult to theoretically produce upper and lower bounds on parameter values. The normal strategy is to choose fairly conservative uniform or Gaussian priors on parameter values. In general, the prior should encapsulate the scale and spread of our current expectation of the parameter value.

Once a prior has been specified, the model $\model$ is complete. The science of the problem is finished, and the rest of the analysis mere statistics. Statistical analysis may be neatly partitioned into two problems: {\em model comparison\/} and {\em parameter estimation}.

\subsection{Model comparison}
It is usually the case in science that there is more than one model available to explain the data. Typically one will have a set of models ${\{\model_1,\model_2,\ldots\}}$, which we wish to scientifically determine the relative merits of given the data $\data$.

We may use the prior to marginalise out all parameter dependence of the each model:
\begin{equation}
  \ev\equiv\Probc{\data}{\model} 
  =
  \int  \Probc{\data}{\params_\model,\model}\Probc{\params_\model}{\model}\:\d{\params_\model}.
  \label{eqn:bay:ev_def}
\end{equation}
This quantity is termed the {\em evidence\/} $\ev$, or {\em marginalised likelihood}, and gives the probability of observing the data $\data$, conditioned on the model $\model$. The quantity we seek however is the probability of each model $\model$ given the data $\data$, which may be obtained using Bayes' theorem:
\begin{equation}
  \Probc{\model_i}{\data} = \frac{\Probc{\data}{\model_i}\Prob{\model_i}}{\Prob{\data}}.
  \label{eqn:bay:bayM}
\end{equation}
In order to utilise this however, we must specify our prior degree of belief in each model:
\begin{equation}
  \Prob{\model_i} = \priorM_i.
\end{equation}
These may have been obtained from previous analyses, but a simple choice would be too choose the models to be equally weighted. The denominator of~\eqref{eqn:bay:bayM} is then a normalising constant, and the posterior degree of belief in each model may be obtained via:
\begin{equation}
  \Probc{\model_i}{\data} 
  \equiv
  \Wmodel_i
  =
  \frac{\ev_i\priorM_i}{\sum_j \ev_j\priorM_j}.
\end{equation}
These model weights $\Wmodel$ may then be used to determine the ``most probable model''. In some cases there is a clear winner, and the other models may be safely discarded, but the more usual scenario is that there are several competing alternatives. It is for this reason that we prefer the term ``model comparison'' to the more oft-quoted {\em model selection}. Additional datasets may determine a clear winner, but in the mean-time the weights $\Wmodel$ can be used to perform proper inference. 

For example, models often predict a distribution for a common derived parameter $\Probc{y}{\model,\data}$.\footnote{e.g.\ in cosmology, different models of the universe will predict an age distribution.} If the data are not strong enough to distinguish a given model, the correct inference on $y$ is to use a posterior which marginalises over all models:
\begin{equation}
  \Probc{y}{\data} 
  = \sum_i\Probc{y}{\data,\model_i}\Probc{\model_i}{\data}
  = \sum_i\Probc{y}{\data,\model_i}\Wmodel_i.
\end{equation}
This fully Bayesian approach has been historically under-utilised due to the difficulties in numerically computing the evidence.



\subsection{Parameter estimation}
Of equal interest to scientists is to ask what the data tells us about the various parameters.  
Bayes' theorem allows us to invert the conditioning in equation~\eqref{eqn:bay:lik_def} and find the {\em posterior\/} $\posterior$ by combining the likelihood~\eqref{eqn:bay:lik_def}, prior~\eqref{eqn:bay:prior_def} and evidence~\eqref{eqn:bay:ev_def}:
%
\begin{equation}
  \posterior\equiv
  \Probc{\params_\model}{\data,\model} = \frac{\Probc{\data}{\params_\model,\model} \Probc{\params_\model}{\model}}{\Probc{\data}{\model}},
  \label{eqn:bay:bayes_theorem}
\end{equation}
%
which is schematically written as:
\begin{align}
  \posterior &= \frac{\lik \times \prior }{\ev },
  \label{eqn:bay:bayes_theorem_abbrv}\\
  \mathrm{Posterior} &= \frac{\mathrm{Likelihood} \times \mathrm{Prior} }{\mathrm{Evidence} }.
\end{align}
This describes how our initial knowledge $\prior$ of the parameters updates to $\posterior$ in light of the data $\data$. Note that the evidence $\ev$ features in both model selection and parameter estimation, and its computation is therefore of great significance.







\section{Numerical statistics: sampling}
\label{sec:bay:samp}
Having discussed the theory of Bayesian statistics, we now turn to the more tricky aspect of actually computing these various inferences. The likelihood~\eqref{eqn:bay:lik_def} $\lik(\params)$ is a routine, if often challenging, quantity to compute. It is the job of observational scientists to provide this function, and for the purposes of inference we may consider it a ``black box''. In general, $\lik$ will be not be analytical, but instead is a numerical and computationally expensive quantity. Any calculation we perform must aim to minimise the number of times we attempt to evaluate $\lik$. The prior $\prior$ is typically much less expensive to compute and is normally expressed using analytic functions such as a uniform or Gaussian distribution.

Typically, in inference calculation we wish to compute quantities that are marginalised over by the posterior. For example means and variances will typically take the form:
\begin{align}
  \mean{f} 
  &= \int f(\params)\posterior(\params)\:\d{\params},
  \label{eqn:bay:mean}\\
  &= \int f(\params)\frac{\lik(\params)\prior(\params)}{\ev}\:\d{\params}.
\end{align}
Given a likelihood $\lik$ and a prior $\prior$, a na\"{\i}ve approach would be to first compute the evidence:
\begin{equation}
  \ev = \int \lik(\params) \prior(\params) \: \d{\params},
  \label{eqn:bay:ev_short}
\end{equation}
and then perform the integral~\eqref{eqn:bay:mean} using a traditional numerical quadrature procedure. In most cases, this method fails at both steps, due to the fact that the dimensionality of the integration is too high for numerical quadrature to succeed.

To see this issue, consider the quadrature integration of a function $g$ along $[0,1]$
\begin{equation}
  \int_0^1 g(x) \d{x} \approx \sum\limits_{i=0}^{n} g(x_i) w_i,
  \label{eqn:bay:quadrature_1d}
\end{equation}
where ${x_i\in[0,1]}$ are the quadrature points ${w_i\in\mathbb{R}}$ are the quadrature weights. For example, if $x_i = \frac{i}{n}$ and $w_i=\frac{1}{n+1}$ then one obtains the left rectangle rule (Figure~\ref{fig:bay:quadrature}). This calculation therefore requires $\bigO{n}$ calculations of the function. In the $d$-dimensional case the generalisation is:
\begin{equation}
  \int_0^1\cdots\int_0^1 g(x_1,\ldots,x_d) \d{x_1}\cdots \: \d{x_d} \approx \sum\limits_{i_1,\ldots,i_d=0}^{n} g(x_{i_1},\ldots,x_{i_d}) w_{i_1,\ldots,i_d},
  \label{eqn:bay:quadrature_nd}
\end{equation}
This calculation however requires $\bigO{n^d}$ function evaluations. This is an exponential scaling with $d$, and is an example of the {\em curse of dimensionality}. Even for modest number of parameters $\left[ d\bigO{6} \right]$ these kind of integrations require unfeasible amounts of computational time. Worse still, for most likelihoods, the region about which $\lik$ is significantly non-zero is much larger than the prior range\footnote{This should be expected, since the good data will update the prior information by some significant amount in each dimension.}. The number of grid points in each dimension $n$ must therefore be taken as reasonably large in order to ensure that enough function evaluations occur at the peak.

\begin{figure}[tp]
  \centering
    \tikzsetnextfilename{1d_left_rectangle}
    \includegraphics[width=0.4\textwidth]{chapter_bayesian_inference/plots/1d_left_rectangle.tikz}
    \tikzsetnextfilename{2d_left_rectangle}
    \includegraphics[width=0.5\textwidth]{chapter_bayesian_inference/plots/2d_left_rectangle.tikz}
  \caption{Approximating an integral using a quadrature rule. In moving a one-dimensional integral (left) to a two-dimensional integral (right), the number of quadrature points becomes exponentially larger. This is an example of the curse of dimensionality.\label{fig:bay:quadrature}}
\end{figure}

Fortunately there is a better way. If one has a set of $n_s$ samples:
\begin{equation}
  S = \left\{ \params_i\sim\posterior:i=1,\ldots,n_s \right\},
  \label{eqn:bay:samples}
\end{equation}
distributed according to the posterior $\posterior$, then we may use the samples to compute the sample average of $f$:
\begin{equation}
  \hat{E}_f = \frac{1}{n_s}\sum\limits_{i=1}^{n_s} f(\params_i).
  \label{eqn:bay:ehat}
\end{equation}
The central limit theorem states that $\hat{E}_f$ is normally distributed with mean $\mean{f}$ and standard deviation $\sigma_f/\sqrt{n_s}$, where $\sigma_f$ is the true standard deviation ${(\mean{f^2}-\mean{f}^2)}^{1/2}$. If $n_s\gg 1$ is large, then $\hat{E}_f$ will closely approximate the true mean. 

This approach has several advantages. If one already has the set $S$ of samples~\eqref{eqn:bay:samples}, the computation~\eqref{eqn:bay:ehat} is extremely cheap, in contrast to a full integral~\eqref{eqn:bay:quadrature_nd}. Further, it has excellent scaling with dimensionality. 

All that remains in order to compute~\eqref{eqn:bay:ehat} is a methodology for cheaply generating samples according to a given distribution $\posterior$. In many cases, one does not even need the normalisation constant $\ev$ in order to sample from $\posterior$, merely an unnormalised posterior $\str{\posterior}= \lik\times\prior$. Suffice to say that this is a well established problem, and for readers unfamiliar with a given sampling method I refer them to Appendix~\ref{chap:sm}.

However, even if one can generate reliably and cheaply a full set $S$ of samples, this is not a full solution to the entire Bayesian inference problem problem. Computing $\mean{f}$ with sampling is only a reasonable methodology when $\sigma_f$ is small (or even defined at all). A very important example of a case where this is not true is the computation of the evidence integral~\eqref{eqn:bay:ev_short}. For these kind of problems we must turn to a more sophisticated methodology



\section{Nested sampling}
\label{sec:bay:nested_sampling}
%
Nested sampling is a methodology devised by \citet{skilling2006} to effectively compute the evidence:
\begin{equation}
  \ev = \int \lik(\params) \prior(\params) \: \d{\params}.
  \tag{\ref{eqn:bay:ev_short} revisited}
\end{equation}
This integral is typically over a high-dimensional parameter space, only a small fraction of which contributes to $\ev$. We may quantify the size of this region by considering the {\em relative entropy\/}\footnote{aka: Kullback-Leibler divergence, information divergence, information gain, KLIC, KL divergence.} of the prior from the posterior distribution:
\begin{equation}
  H = -\int \log\left( \frac{\posterior(\params)}{\prior(\params)}\right) \posterior(\params)\: \d{\params}.
  \label{eqn:bay:rel_ent}
\end{equation}  
This quantifies the number of nats\footnote{if ``bits'' are a measure of information in base 2, then ``nats'' is information in base $e$.} of information that have been gained on account of the data. Equivalently, the fraction of the prior which contributes non-negligibly to~\eqref{eqn:bay:ev_short} is approximately $X\sim e^{-H}$.

The size and position of the region surrounding the peak (or peaks) will not be known {\em a priori}, and in high dimensions is challenging to find.  

\subsection{The prior volume transformation $\lik(X)$}
The first trick is to transform~\eqref{eqn:bay:ev_short} into an effectively one dimensional problem. We define an iso-likelihood contour as:
\begin{equation}
  C(\liks) = \{\params : \lik(\params)=\liks \}.
  \label{eqn:bay:contour_def}
\end{equation}
Each contour defined by $\liks$ will also enclose a $\params$-region containing some fraction of the prior:
\begin{equation}
  X(\liks) = \int_{\lik(\params)>\liks} \prior(\params) \d{\params},
  \label{eqn:bay:prior_volume}
\end{equation}
which is termed the {\em prior volume\/} by physicists and {\em prior mass\/} by mathematicians. For each contour $C$, there corresponds a likelihood and a prior volume. We may use this to re-write the likelihood as a function $\lik(X)$.\footnote{More rigorously, one may invert equation~\protect\eqref{eqn:bay:prior_volume} since $X(\lik)$ is monotonic.} and thus the integral~\eqref{eqn:bay:ev_short} as:
\begin{equation}
  \ev = \int_0^1 \lik(X)\:\d{X}.
  \label{eqn:bay:ev_X}
\end{equation}
This process is graphically depicted in Figure~\ref{fig:bay:prior_volume} for a two dimensional likelihood. Note the limits are from $0$ to $1$ since $X$ denotes the {\em fraction\/} of the prior enclosed by the likelihood contour\footnote{or alternatively because $X$ is a form of cumulative distribution function}.
%
\begin{figure}[tp]
  \centering
  \includegraphics[width=\textwidth]{chapter_bayesian_inference/figures/contour_nested}
  \caption{%
    The nested sampling volume transformation.
    Left: five iso-likelihood contours of a two-dimensional multi-modal likelihood function $\lik(\params)$. Each contour encloses some fraction of the prior $X$, indicated by colour.
    Right: Likelihood $\lik$ as a function of the volume $X$ enclosed by the contour. The evidence is the area under this curve.\label{fig:bay:prior_volume}
  }
\end{figure}

An inverse derivation is to examine~\eqref{eqn:bay:ev_short}, and define a new integration variable $dX = \prior(\params)\:d\params$. From this definition, the notion of prior volume~\eqref{eqn:bay:prior_volume} follows, which may be inverted to give $\lik(X)$. Mathematically inclined readers may like to consider how this procedure is related to Lesbesque integration.

The beauty of this process is that it completely removes all complications such as geometry, topology and dimensionality, reducing~\eqref{eqn:bay:ev_X} to a one dimensional integral.  We may now evaluate the evidence using standard quadrature techniques:
\begin{equation}
  \ev \approx \sum_{i} w_i \lik(X_i),
  \label{eqn:bay:quadrature}
\end{equation}
where $\{X_i\}$ are the quadrature points and $\{w_i\}$ are the quadrature weights. 

\subsection{Computing the prior volumes}

This is all well and good, but so far all we have done is transfer the complications (geometry, topology and dimensionality) from the evidence integral~\eqref{eqn:bay:ev_short} to the integral which calculates the prior volume~\eqref{eqn:bay:prior_volume}. John's true genius is to come up with a scheme that allows one to {\em estimate\/} the prior volume $X$ in a quantifiable way.

The method relies on having a sampling procedure capable of producing a sample $\tilde{\params}$ distributed according to the prior $\prior$, subject to the constraint that it lies within the likelihood contour $C(\liks)$.  This is termed ``sampling within a hard-edged likelihood constraint''.  In theory shouldn't be any more challenging than traditional sampling techniques such as Metropolis-Hastings \citep{skilling2006}. 

Importantly, a sample within $C(\liks)$ distributed according to the prior $\prior$ will have its prior volume $X$ drawn {\em uniformly\/} within $[0,X_\ast]$, $X_\ast=X(\liks)$. This demonstrates another reason why the prior volume is a powerful variable to work with.

\subsection{Single-point nested sampling}

If one draws an initial sample from $C(0)$ (i.e.\ the entire space) according to the prior $\prior$, one will have generated a point with prior volume $X_1$ and likelihood $\lik_1$. On average, a single sample will cut the prior in two, and the mean $\mean{X_1}=\frac{1}{2}$. At this value of $X_1$, it is unlikely that the likelihood is large enough to contribute to the evidence integral ($X_1\gg e^{-H}$) so we should go deeper.

If one then generates $X_2$ by sampling from the contour defined by the previous point $C(\lik_1)$, then $X_2=t_2\times X_1$, where $t$ is a uniform variable drawn from $[0,1]$. We thus find that the on average the second sample will have a mean $\mean{X_2} = \frac{1}{4}$. 

Repeating this process by using the previous contour as the hard likelihood constraint will mean that we sample within regions closer and closer to the peak, with $\mean{X_i} = 2^{-i}$. When $X_i\sim e^{-H}$, then the samples with these quadrature points start contributing to the evidence integral. Eventually, the samples compress too far ($X_i\ll e^{-H}$) and no longer contribute to the evidence, and the algorithm should stop.

More precisely, one finds that:
\begin{equation}
  X_i = \prod_{j=1}^i t_j,
  \label{eqn:bay:X_i}
\end{equation}
where each $t_j$ is an independent random variable drawn uniformly from $[0,1]$. As $i$ increases, $X_i$ becomes distributed {\em log-normally\/} with:
\begin{align}
  \log X_i  &\approx -i \pm \sqrt{i},
  \label{eqn:bay:log_normal_1_1}
  \\
  P(X_i) &= \frac{1}{\sqrt{2\pi i} X}\exp\left[ -\frac{{\left( \log X_i - i \right)}^2}{2 i}  \right],
  \label{eqn:bay:log_normal_1_2}
\end{align}
This follows by taking logarithms of~\eqref{eqn:bay:X_i} and applying the central limit theorem. This is particularly impressive, since one finds that the samples compress the prior space {\em exponentially}. In general, the logarithmic compression from the prior to the posterior $H$ tends to be linear in dimensionality $d$, so single point nested sampling scales linearly with dimensionality.

As this scheme progresses, one is left with a set of likelihoods $\lik_i$, each paired with a volume $X_i$. These volumes are not deterministically known, but defined by the probabilistic definition~\eqref{eqn:bay:X_i}. The evidence may be approximated as:
\begin{equation}
  Z = \sum_{i=1}^n \lik_i \times (X_{i-1}-X_{i}),
  \label{eqn:bay:ev_sum}
\end{equation}
where for simplicity the quadrature weighting is taken as $w_i = X_{i-1}-X_i$. This means that the evidence is also implicitly probabilistic. Instead of computing an exact evidence, one may infer the distribution of evidence values that it could take.

In general, single point nested sampling will not produce a particularly accurate evidence estimation, but we can go one better.


\subsection{Multi-point nested sampling}
Instead of working with a single sample, one could initially generate a set of $\nlive$ ``live points'' distributed across the prior. The lowest likelihood point will have likelihood $\lik_1$ and a volume $X_1 = t$ where $t$ is distributed as the largest of $\nlive$ uniform samples in $[0,1]$. Elementary probability shows that:
\begin{equation}
  P(t) = \nlive t^{\nlive-1}.
  \label{eqn:bay:t_prob}
\end{equation}
If this point is deleted, and replaced with a new point drawn from the contour defined by $\lik_1$, then one will still have $\nlive$ points, now uniformly distributed uniformly in $[0,X_1]$. Proceeding onward, one finds that the volume becomes log-normally distributed:
\begin{align}
  \log X_i  &\approx -\frac{i}{\nlive} \pm \sqrt{\frac{i}{\nlive}}.
  \label{eqn:pc:X_full}
\end{align}
The prior space compression is slower than single point nested sampling, but with a correspondingly lower error. This therefore produces a more accurate inference on the evidence.

The deleted point is then added to a list of ``dead points'' which can then be used to make inferences on the evidence using equation~\eqref{eqn:bay:ev_sum}.


\subsection{Posterior inference}
Nested sampling is designed to calculate the evidence, but it also produces posterior samples as a by-product of the calculation. Samples can be obtained by sampling randomly under the posterior curve.% (Figure~\ref{fig:bay:posterior}). 
Since we have partitioned the area into regions of size $\lik_i w_i$, we can use this as an importance weighting.

%\begin{figure}[tp]
%  \centering
%  \tikzsetnextfilename{posterior_plot}
%  \includegraphics[width=\textwidth]{chapter_bayesian_inference/plots/posterior.tikz}
%  \caption{%
%    Posterior samples.\label{fig:bay:posterior}
%}
%\end{figure}

Thus, dead points may be used as posterior samples with importance weighting:
\begin{equation}
  p_i = \frac{\lik_i w_i}{\ev},
  \label{eqn:bay:posterior_weight}
\end{equation}
where the additional factor of $\ev$ merely ensures that they sum to $1$ for consistency.



\subsection{Terminating the algorithm}
One does not wish to continue compressing the space once $X_i\ll e^{-H}$ for two reasons. The first reason is that this will not give any further accuracy on the evidence calculation. The second is more important. The region beneath $X_i\ll e^{-H}$ is not statistically relevant. Scientists used to maximum likelihood methods find this difficult to swallow, and often desire algorithms that will maximise the function to find the highest peak. In high dimensions, the absolute peak occupies such a vanishingly small percentage of the prior mass that it is totally irrelevant for the purposes of inference. In the case of a Gaussian likelihood, the peak value is a useful {\em summary statistic\/} but nothing more than that. Otherwise, one should not concern oneself with maximum likelihood, only in an accurate description of the peak provided by a set of samples.

%
\begin{figure}[tp]
  \centering
  \includegraphics[width=\columnwidth]{chapter_bayesian_inference/figures/gaussian_weight}
  \caption{%
    Plot of a generic likelihood as a function of the prior volume $\lik(X)$. In high dimensions, the likelihood is only visible if plotted against $\log X$ (dashed curve). However, the evidence is better visualised by plotting $X\log(X)$ (solid curve). The area under the solid curve corresponds to the evidence. The magnitude of the solid curve is proportional to the importance weighting. Nested sampling proceeds from high to low volumes. After some time, the live points no longer contribute significantly to the evidence, and the algorithm terminates at this point.\label{fig:bay:gaussian_weight}
  }
\end{figure}
%
As nested sampling proceeds, the likelihoods $\lik_i$ monotonically increase, but the weights $w_i$ monotonically decrease. This results in a peak in importance weights~\eqref{eqn:bay:posterior_weight} that can be seen in Figure~\ref{fig:bay:gaussian_weight}. We terminate the algorithm once the remaining posterior mass (white region) left in the live points is some small fraction of the currently calculated evidence (dark region). The posterior mass left in the live points at iteration $i$ can be estimated by:
\begin{equation}
  \ev_\slive \approx \mean{\lik}_\slive X_i,
  \label{eqn:bay:live_evidence}
\end{equation}
where the average is taken over the live points. Since this is typically an underestimate at early times, this will not cause premature termination.



\subsection{Nested sampling: summary}
\label{sec:bay:comp_space}
\begin{itemize}
  \item The algorithm initialises by sampling $\nlive$ points from the prior distribution $\prior(\params)$. 
  \item At iteration $i$, the point with the lowest likelihood $\lik_i$ is deleted, and then replaced by a new point, which is drawn from the prior subject to the constraint that its likelihood is greater than $\lik_i$. 
  \item The evidence may be inferred from the dead points via:
    \begin{align}
      \ev &= \sum_{i=1}^n w_i \lik_i, &
      X_i &= \prod_{j=1}^{i} t_j, \nonumber \\
      w_i &= X_{i-1}-X_{i},  & 
      t_j &\sim U[0,1].\nonumber 
    \end{align}
  \item The algorithm terminates when:
    \begin{equation}
      X_i \times \mean{\lik}_\slive \ll \ev.
    \end{equation}
  \item A set of posterior samples may be produced using the dead points with importance weighting:
    \begin{equation}
      p_i = \frac{w_i\lik_i}{\ev}.\nonumber
    \end{equation}

\end{itemize}


\section{Conclusion}
The following two chapters are concerned with describing my contributions to nested sampling theory and implementation in the algorithm \PolyChord{}. After that I describe a novel numerical approach to solving differential equations which is relevant for my work in the field of quantum cosmology.



\cleardoublepage{}
